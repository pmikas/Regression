{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px\">Model Training - Linear Regression</h1>\n",
    "<hr>\n",
    "\n",
    "1. Split the dataset\n",
    "2. Build model pipelines\n",
    "3. Declare hyperparameters o tune\n",
    "4. Fit and tune models with cross-validation\n",
    "5. Evaluate metrics and select winner\n",
    "6. Saving the winning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**Import libraries**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for Dataframes\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',100)\n",
    "\n",
    "# Matplolib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-Learn for modeling\n",
    "import sklearn\n",
    "\n",
    "# Pickle for saving model files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Function for standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Elastic Net, Ridge Regression and Lasso Regression\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "\n",
    "# Import Random Forest and Gradient Boosted Trees\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Import r2_score and mean_absolute_error functions\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**Load analytical base table**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analytical base table from Feature Engineering\n",
    "df = pd.read_csv('analytical_base_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**1. Split the dataset**</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the dataframe into separate objects for the target variable (y) and the input features (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate object for target variable\n",
    "y = df.price\n",
    "\n",
    "# Create separate object for input features\n",
    "X = df.drop('price', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training sets** are used to fit and tune the models<br>\n",
    "**Test sets** are put aside as unseen data to evaluate your models\n",
    "<br>\n",
    "* Split the train and test set, passing in the argument **test_size = 0.2** to set aside 20% of the observations for the test set\n",
    "* The **random_state = 1234** is set for replicable results\n",
    "* **Important**: For classification model also pass in the argument **stratify = df.target** in order to make sure the target variable's classes are balanced in each subset of data. This is **stratified random sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43136 10784 43136 10784\n"
     ]
    }
   ],
   "source": [
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)\n",
    "\n",
    "# Print number of observations in X_train, X_test, y_train, and y_test\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**2. Build model pipelines**</span><br>\n",
    "The pipeline will standardize the data first, then apply the model algorithm to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**: should be performed inside the cross-validation loop\n",
    "* Transform or scale the features\n",
    "* Perform automatic feature reduction (e.g. PCA)\n",
    "* Remove correlated features<br>\n",
    "<br>\n",
    "**Standartization**: transforms all features to the same **scale** by substracting means and dividing by standard deviations.\n",
    "* Feature's distribution **centered around zero, with unit variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **random_state = 123** is set for replicable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines dictionary\n",
    "pipelines = {'lasso': make_pipeline(StandardScaler(), Lasso(random_state = 123)),\n",
    "             'ridge': make_pipeline(StandardScaler(), Ridge(random_state = 123)),\n",
    "             'enet': make_pipeline(StandardScaler(), ElasticNet(random_state = 123)),\n",
    "             'rf': make_pipeline(StandardScaler(), RandomForestRegressor(random_state = 123)),\n",
    "             'gb': make_pipeline(StandardScaler(), GradientBoostingRegressor(random_state = 123))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**3. Declare hyperparameters to tune**</span><br>\n",
    "**Hyperparameters** express \"higher-level\" structural settings for modeling algorithms<br>\n",
    "* e.g. strength of the penalty used in regularized regression\n",
    "* e.g. the number of trees to include in a random forest\n",
    "* They are **decided** before training the model because they cannot be learned from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the keys that begin with 'lasso__' are hyperparameters for Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('lasso', Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=123,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False))],\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'lasso': Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "    normalize=False, positive=False, precompute=False, random_state=123,\n",
       "    selection='cyclic', tol=0.0001, warm_start=False),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'lasso__alpha': 1.0,\n",
       " 'lasso__copy_X': True,\n",
       " 'lasso__fit_intercept': True,\n",
       " 'lasso__max_iter': 1000,\n",
       " 'lasso__normalize': False,\n",
       " 'lasso__positive': False,\n",
       " 'lasso__precompute': False,\n",
       " 'lasso__random_state': 123,\n",
       " 'lasso__selection': 'cyclic',\n",
       " 'lasso__tol': 0.0001,\n",
       " 'lasso__warm_start': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of tuneable hyperparameters of Lasso pipeline\n",
    "pipelines['lasso'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regularized regression, the most impactful hyperparameter is the alpha(**strength of the penalty**)\n",
    "* Also known as lambda\n",
    "* **alpha** is a positive value, typically between 0 and 10\n",
    "* The default value is 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso hyperparameters\n",
    "lasso_hyperparameters = {'lasso__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge hyperparameters\n",
    "ridge_hyperparameters = {'ridge__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **l1_ratio** is the ratio of **L1** penalty to **L2** penalty\n",
    "* The default value is 0.5\n",
    "* When l1_ratio = 1, it is Lasso regression\n",
    "* When l1_ratio = 0, it is Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net hyperparameters\n",
    "enet_hyperparameters = {'elasticnet__alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n",
    "                        'elasticnet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **n_estimators** is the **number of decision trees** to include in the random forest\n",
    "* Usually, more is better\n",
    "* The default value is 10, which is usually too few\n",
    "* Try 100 and 200<br>\n",
    "<br>\n",
    "* **max_features** controls the number of features each tree is allowed to choose from\n",
    "* It's what allows your random forest to perform feature selection\n",
    "* The default value is 'auto', which sets max_features = n_features\n",
    "* 'sqrt' sets max_features = sqrt(n_features)\n",
    "* 0.33 sets max_features = 0.33*n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest hyperparameters\n",
    "rf_hyperparameters = {'randomforestregressor__n_estimators': [100, 200],\n",
    "                     'randomforestregressor__max_features': ['auto', 'sqrt', 0.33]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **n_estimators** same as for Random Forest\n",
    "* Try 100 and 200<br>\n",
    "<br>\n",
    "* **learning_rate** shrinks the contribution of each tree\n",
    "* There is a tradeoff between learning rate and number of trees\n",
    "* The default value is 0.1\n",
    "* Try 0.05, 0.1 and 0.2<br>\n",
    "<br>\n",
    "* **max_depth** controls the maximum depth of each tree\n",
    "* The default value is 3\n",
    "* Try 1, 3 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tree hyperparameters\n",
    "gb_hyperparameters = {'gradientboostingregressor__n_estimators': [100, 200],\n",
    "                      'gradientboostingregressor__learning_rate': [0.05, 0.1, 0.2],\n",
    "                      'gradientboostingregressor__max_depth': [1, 3, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'lasso': lasso_hyperparameters,\n",
    "                   'ridge': ridge_hyperparameters,\n",
    "                   'enet': enet_hyperparameters,\n",
    "                   'rf': rf_hyperparameters,\n",
    "                   'gb': gb_hyperparameters}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**4. Fit and tune models with cross-validation**</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearchCV function performs cross-validation on the **hyperparameter grid**, through each **combination of values**. It then calculates **cross-validated scores** (using performance metrics) for each combination of hyperparameter values and picks the combination that has the best score\n",
    "* **cv** is the number of cross-validation folds\n",
    "* **n_jobs = -1** trains in parallel across the maximum number of cores of the computer, speeding it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso has been fitted.\n",
      "ridge has been fitted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enet has been fitted.\n",
      "rf has been fitted.\n",
      "gb has been fitted.\n"
     ]
    }
   ],
   "source": [
    "# Create empty dictionary called fitted_models\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "for name, pipeline in pipelines.items():\n",
    "    \n",
    "    # Create cross-validation object from pipeline and hyperparameters\n",
    "    model = GridSearchCV(pipeline, hyperparameters[name], cv = 10, n_jobs = -1)\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Store model in fitted_models[name]\n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    # Print when the model is fitted\n",
    "    print(name, 'has been fitted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**5. Evaluate models and select winner**</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso 0.919009382798393\n",
      "ridge 0.9184615703596563\n",
      "enet 0.9188422102370558\n",
      "rf 0.9806322144008012\n",
      "gb 0.9808295698775237\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_ for each fitted_model\n",
    "for name, model in fitted_models.items():\n",
    "    print(name, model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **regression problems**, the default scoring metric is the average **R²** on the holdout folds\n",
    "* **R²** is the \"percent of variation in the target variable that can be explained by the model\"\n",
    "* Because is the average R² from the **holdout folds**, higher is almost always better<br>\n",
    "<br>\n",
    "* **MAE** (Mean Absolute Error) is the average absolute difference between predicted and actual values for our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso\n",
      "----------\n",
      "R^2:  0.9195452051901299\n",
      "MAE:  742.7924726851604\n",
      "\n",
      "ridge\n",
      "----------\n",
      "R^2:  0.9201148406728155\n",
      "MAE:  738.4799951489126\n",
      "\n",
      "enet\n",
      "----------\n",
      "R^2:  0.9192310296831971\n",
      "MAE:  750.1782207428162\n",
      "\n",
      "rf\n",
      "----------\n",
      "R^2:  0.9820270151300432\n",
      "MAE:  264.8424436323212\n",
      "\n",
      "gb\n",
      "----------\n",
      "R^2:  0.9816126039235502\n",
      "MAE:  278.52906986162003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through model pipelines, predicting and calculating R^2 and MAE\n",
    "for name, model in fitted_models.items():\n",
    "    \n",
    "    # Predict test set using the fitted models\n",
    "    pred = model.predict(X_test)\n",
    "    print(name)\n",
    "    print('----------')\n",
    "    \n",
    "    # Calculate and print R^2 and MAE\n",
    "    print('R^2: ', r2_score(y_test, pred))\n",
    "    print('MAE: ', mean_absolute_error(y_test, pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**6. Saving the winning model**</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=0.33, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "           oob_score=False, random_state=123, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selected winning hyperparameters\n",
    "fitted_models['rf'].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_model.pkl', 'wb') as f:\n",
    "    pickle.dump(fitted_models['rf'].best_estimator_, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
